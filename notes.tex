\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{amsmath}

\def\Var{{\textrm{Var}}\,}
\def\Det{{\textrm{Det}}\,}
\def\E{{\textrm{E}}\,}
\def\Cov{{\textrm{Cov}}\,}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{ Notes on Stein's Ambiguity Function Processing}
\author{ Chris Hall }
\date{\today}

\begin{document}
\maketitle  
%\pagebreak

% Optional TOC
% \tableofcontents
% \pagebreak

%--Paper--

%\section{Section 1}


\section{Introduction}

These notes are intended to document my thoughts on Seymour Stein's 1981 paper titled "Algorithms for ambiguity function processing."

Section two of these notes contains the missing derivation for the Time of Arrival estimator variance stated in Stein's Algorithms for Ambiguity Function processing.

The remaining sections deal with Stein's definition of effective SNR, and how effective SNR changes depending on the detection method employed.
To correctly calculate the effective SNR, the signal power and noise power of the received signals must be correctly propagated through every step of a detection system.

Three detection methods will be analyzed.

\begin{equation}
r = \sum s_1(t) s_2^*(t)
\end{equation}

\begin{equation}
r = |\sum s_1(t) s_2^*(t)|
\end{equation}

\begin{equation}
r = \sum s_1(t)s_1(t-1) s_2^*(t) s_2^*(t-1)
\end{equation}


\section{Derivation of Stein's Time of Arrival Accuracy and the Maximum Likelihood Estimator}

Most of this derivation is pieced together from chapters $6$ and $10$ of Robert McDonough's "Detection of Signals in Noise" second edition.

We start by defining a noisy received signal:


\begin{equation}
r(t) = s_r(t|x) + n(t)
\end{equation}

For the case where $n(t)$ is additive white Gaussian noise, the probability distribution of $r(t)$ is

\begin{equation}
p_i(r)
=
\frac{(2\pi)^\frac{m}{2}}{\sqrt{\Det(C)}}
\exp \left[ -\frac{1}{2} (r-s_i)^T C^{-1} (r-s_i) \right]
\end{equation}

$C$ is the covariance matrix for the random noise component, which will be diagonal: $\frac{2\Delta t}{N_0}$.  The sample period is $\Delta t$.

\begin{equation}
p(r)
=
\left( \frac{\Delta t}{\pi N_0} \right)^\frac{m}{2}
\exp \left[ -\frac{\Delta t}{N_0} \sum (r - s_r)^2 \right]
\end{equation}

Where $m$ is the number of samples in the signal and the summation is taken over the $m$ samples. 
To get the continuous time signal model, we will take the limit as sample period, $\Delta t$, goes to zero.

\begin{equation}
p(r) = \exp \left[ -\left(\frac{1}{N_0}\right) \int^T_0 \left[ r(t) - s_r(t) \right]^2 dt \right]
\end{equation}

Here $x$ represents any unknown parameters in the signal, $s$. These unknown parameters could be amplitude, phase, or time offset. In our case we are interested in examining signals with an unknown time offset.

\begin{equation}
\bar{r}(t) = E[r(t)] = s_r(t|x)
\end{equation}

The likelihood function will be

\begin{equation}
p(r|x) 
=
\alpha \exp \{ - \left( \frac{1}{N_0} \right) \int^T_0 \left[ r(t) - s_r(t|x) \right]^2 dt \}
\end{equation}


The maximum likelihood estimate comes from setting the derivative of the log likelihood function equal to zero.

\begin{equation}
\frac{\partial \left[ \ln p(r|x) \right]}{\partial x}
= 0
\end{equation}

\begin{equation}
\frac{2}{N_0} \int^T_0 \left[ r(t) - s_r(t|x)\right]
\left[ \frac{\partial s_r(t|x)}{\partial x} \right] dt = 0
\end{equation}

Note that $r(t) - s_r(t|x) = n(t)$ and $E[n(t)n(t')]=R_n(t-t')=(N_0/2)\delta(t-t')$.

\begin{equation}
s_r(t|x) = s_r(t|\tau) = s_r(t-\tau)
\end{equation}


\begin{equation}
\int^\frac{T}{2}_\frac{-T}{2}
 \left[ r(t) - s_r(t-\tau)\right]
\left[ \frac{\partial s_r(t-\tau)}{\partial x} \right] dt = 0
\end{equation}

The second term reduces to zero.

\begin{equation}
- \int^\frac{T}{2}_\frac{T}{2}
s_r(t-\tau)
\left[ \frac{\partial s_r(t-\tau)}{\partial \tau} \right] dt
\end{equation}


\begin{equation}
=-\int^{\frac{T}{2}-\tau}_{\frac{-T}{2}-\tau}
s_r(u)
\left[ \frac{\partial s_r(u)}{\partial u} \right] du
\end{equation}

\begin{equation}
=\frac{1}{2} \int^{\frac{T}{2}-\tau}_{\frac{-T}{2}-\tau}
\left[ \frac{\partial s_r^2(u)}{\partial u} \right] du
\end{equation}

\begin{equation}
=\frac{1}{2}[s_r^2(T/2-\tau) - s_r^2(-T/2-\tau)] = \frac{1}{2}[s_r^2(T/2) - s_r^2(-T/2)] = 0
\end{equation}

The maximum likelihood estimate, $\hat{\tau}$ , is the value of $\tau$ for which

\begin{equation}
\int^\frac{T}{2}_\frac{-T}{2}
r(t)
\left[ \frac{\partial s_r(t-\tau)}{\partial \tau} \right] dt = 0
\end{equation}


\subsection{Fisher Information and Estimator Variance}


The Cramer Rao lower bound is a lower bound on the variance of an estimator, given by:

\begin{equation}
\Cov(\hat{x}(y))
\geq
\left[I + \frac{\partial b(x)}{\partial x}\right]^T F^{-1}(x) \left[I + \frac{\partial b(x)}{\partial x}\right]
\end{equation}

Where $F$ is the Fisher information matrix. For an unbiased estimator, the bias term $b(x)$ will be zero, and the equation simplifies to

\begin{equation}
\Cov(\hat{x}(y))
\geq
F^{-1}(x)
\end{equation}

In our case the Fisher information matrix consists of only one element since there is only one unknown variable to be estimated, $\tau$. The Fisher information matrix is calculated as

\begin{equation}
F(x) = E\left[ \frac{\partial \ln [p(y|x)]}{\partial x} \right]^T
\left[ \frac{\partial \ln [p(y|x)]}{\partial x} \right]
\end{equation}

\begin{equation}
\frac{\partial \ln [p(y|x)]}{\partial x} 
=
\int^\frac{T}{2}_\frac{-T}{2}
\left[ r(t) - s_r(t) \right]
\left[ \frac{\partial s_r(t-\tau)}{\partial \tau} \right] dt
\end{equation}

Note that $r(t) - s_r(t|x) = n(t)$ and $E[n(t)n(t')]=R_n(t-t')=\frac{N_0}{2}\delta(t-t')$.


The Fisher information matrix has elements

\begin{equation}
F_{ij}
=
\left( \frac{2}{N_0} \right)^2
\int^\frac{T}{2}_\frac{-T}{2} \int^\frac{T}{2}_\frac{-T}{2}
\left( \frac{N_0}{2} \right)
\delta (t - t')
\left[ \frac{\partial s_r(t|x)}{\partial x_i} \right]
\left[ \frac{\partial s_r(t|x)}{\partial x_j} \right]
dt dt'
\end{equation}

\begin{equation}
F_{ij}
=
\frac{2}{N_0}
\int^\frac{T}{2}_\frac{-T}{2}
\left[ \frac{\partial s_r(t|x)}{\partial x_i} \right]
\left[ \frac{\partial s_r(t|x)}{\partial x_j} \right]
dt
\end{equation}

There is only one unknown, $\tau$, that we are interested in estimating, so our Fisher information matrix will only have one element, $\tau = x_i = x_j$. Now we can substitute $\tau$ for $x$ since we are trying to estimate the time delay.


The Cramer Rao Lower Bound is

\begin{equation}
\Var(\hat{t} - \tau)
\geq
\left[
\frac{2}{N_0}
\int^\frac{T}{2}_\frac{-T}{2} \left[ \frac{\partial s_r(t-\tau)}{\partial \tau} \right]^2 dt \right]^{-1}
\end{equation}


\begin{equation}
= \left[ \frac{2}{N_0} \int^\frac{T}{2}_\frac{-T}{2} \left[ \frac{\partial s_r(t)}{\partial t} \right]^2 dt \right]^{-1}
\end{equation}

Applying Parseval's theorem, we get

\begin{equation}
\Var(t-\tau)
\geq
\left[ \frac{2}{N_0} \int^{\infty}_{-\infty} \omega^2 |S_r(i\omega )|^2 df \right]^{-1}
\end{equation}

Received signal energy is

\begin{equation}
E_r = \int^\frac{T}{2}_\frac{-T}{2} s_r^2(t)dt = \int^\infty_{-\infty} |S_r(i\omega)|^2 df
\end{equation}

\begin{equation}
\beta^2 = \frac{1}{E_r} \int^\infty_{-\infty} \omega^2 |S_r(i\omega)|^2 df
\end{equation}

\begin{equation}
\Var(\hat{\tau} - \tau)
\geq
\frac{1}{(2E_r/N_0)\beta^2}
\end{equation}

The noise power is given as $\sigma_n^2 = \frac{N_0B}{2}$. Signal power is $P_r = \frac{E_r}{T}$. SNR will then be $\gamma = \frac{2E_r}{N_0BT}$

\begin{equation}
\boxed{
\Var(\hat{\tau} - \tau)
\geq
\frac{1}{\gamma BT\beta^2}
}
\end{equation}


\section{Time of Arrival Accuracy in Discrete Time}

\begin{equation}
p(r) = \left( \frac{\Delta t}{\pi N_0}\right)^{\frac{m}{2}}
\exp \{ \frac{-\Delta t}{N_0} \sum (r(t) - s_r(t))^2\}
\end{equation}

\begin{equation}
p(r|x) = \left( \frac{\Delta t}{\pi N_0}\right)^{\frac{m}{2}}
\exp \{ \frac{-\Delta t}{N_0} \sum (r(t) - s_r(t|x))^2\}
\end{equation}

\begin{equation}
\frac{\partial \ln (p(r|x))}
{\partial x}
= 0
\end{equation}

\begin{equation}
\frac{m}{2}
\ln \left( \frac{\Delta t}{\pi N_0} \right)
-
\frac{\Delta t}{N_0}
\sum (r(t) - s_r(t|x))^2
\frac{\partial}{\partial x}
= 0
\end{equation}

\begin{equation}
\frac{-2 \Delta t}{N_0}
\sum (r(t) - s_r(t|x))
\left( \frac{\partial s_r(t|x)}{\partial x} \right) = 0
\end{equation}

Recall that $s_r(t|x)$ is actually $s_r(t-\tau)$

\begin{equation}
\frac{-2 \Delta t}{N_0}
\sum (r(t) - s_r(t-\tau))
\left( \frac{\partial s_r(t-\tau)}{\partial \tau} \right) = 0
\end{equation}

The second term in the equation above equals $0$.

\begin{equation}
\frac{-2 \Delta t}{N_0}
\sum r(t) \frac{\Delta s_r(t-\tau)}{\partial \tau}
= 0
\end{equation}

\subsection{Fisher Information and Estimator Variance}

TODO


\section{Matched Filter Detection}

Stein defines the output of a correlation at a time lag, $\tau=0$, as

\begin{equation}
r(t) = \sum s_1(t)s^{*}_2(t)
\end{equation}


We want to determine the SNR of the correlated signal. First let's start with something simple: a single sample. Note this assumption does not affect our final results. For sequences of a generalized length we would substitute $E_s = \sum s^2$ for $s^2$ in the derivations outlined below. The correlation then becomes

\begin{equation}
y = (s_1 + n_1)(s_2^* + n_2)
\end{equation}

\begin{equation}
y = s_1 s^*_2 + s_1 n_2 + s_2 n_1 + n_1 n_2
\end{equation}

The first term is the signal term and the remaining three terms are noise terms. The SNR will be given as

\begin{equation}
\gamma = \frac{|s_1 s_2|^2}{\sigma_{n1}^2 |s_1|^2 + \sigma_{n2}^2 |s_2|^2 + \sigma_1^2 \sigma_2^2}
\end{equation}

The third term in the denominator is not obvious. I'm claiming that the product of two random variables has a variance equal to the product of the variances of the two random variables.

\begin{equation}
var (n_1 n_2) = \E[n_1^2 n_2^2 - \E[n_1 n_2]^2]
\end{equation}

If both $n_1$ and $n_2$ have a mean of zero then $\E[n_1 n_2] = 0$.

\begin{equation}
var(n_1 n_2) = \E[n_1^2 n_2^2] = \E[n_1^2] \E[n_2^2] = \sigma_1^2\sigma_2^2
\end{equation}

Returning to the original problem, we can invert the fraction and decompose it into three terms.

\begin{equation}
\frac{1}{\gamma}
=
\frac{\sigma_{n1}^2 |s_1|^2 + \sigma_{n2} |s_2|^2 + \sigma_1^2 \sigma_2^2}{|s_1|^2 |s_2|^2}
\end{equation}

\begin{equation}
\frac{1}{\gamma}
=
\frac{\sigma_1^2 |s_1|^2}{|s_1|^2 |s_2|^2}
+
\frac{\sigma^2_2 s_2^2}{|s_1|^2 |s_2|^2}
+
\frac{\sigma_1^2 \sigma_2^2}{|s_1|^2 |s_2|^2}
\end{equation}

\begin{equation}
\frac{1}{\gamma}
=
\frac{\sigma_1^2}{|s_2|^2}
+
\frac{\sigma^2_2}{|s_1|^2}
+
\frac{\sigma_1^2 \sigma^2_2}{|s_1|^2 |s_2|^2}
\end{equation}

\begin{equation}
\boxed{
\frac{1}{\gamma}
=
\frac{1}{\gamma_1}
+
\frac{1}{\gamma_2}
+
\frac{1}{\gamma_1 \gamma_2}
}
\end{equation}

If $s_1(t) = \alpha s_2(t)$ then

\begin{equation}
r = \sum s_1(t)s^{*}_2(t)
\end{equation}

will be purely real. The noise will still be complex, but only the real component of the noise will degrade our autocorrelation measurement. The real component of complex Gaussian noise contains half of the power of the complex Gaussian noise. This affects our effective SNR.

\begin{equation}
\boxed{
\frac{1}{\gamma}
=
\frac{1}{2}
\left(
\frac{1}{\gamma_1}
+
\frac{1}{\gamma_2}
+
\frac{1}{\gamma_1 \gamma_2}
\right)
}
\end{equation}

This one half factor should only be used if we are dealing with complex signals and we know the phase relationship between them. If the original signals are real, we get no gain from looking at only the real component, and the one half factor should not be used. If we have signals with an unknown phase relationship, and our detection function involves taking a magnitude, then we can not benefit from coherent detection and the one half factor should not be used.

\section{Non-Coherent Matched Filter Detection}

\begin{equation}
r(t) = \sum |s_1(t)s_2(t)|
\end{equation}

\begin{equation}
\gamma = \frac{|s_1 s_2|^2}{var(|s_1 n_2 + s_2 n_1 + n_1 n_2|)}
\end{equation}


The first two terms in the denominator are Rayleigh distributed with variance

\begin{equation}
var(|s_1 n_2|) =
\frac{4-\pi}{2} |s_1|^2 \sigma_{n2}^2
\end{equation}


\begin{equation}
var(|s_1 n_2 + s_2 n_1|) =
\frac{4-\pi}{2}
\left(|s_1|^2 \sigma_{n2}^2 + |s_2|^2 \sigma_{n1}^2 \right)
\end{equation}

For a minute let's claim that the third term in the denominator is insignificant compared to the other two, due to some assumption about the relative powers of $n_1$ and $n_2$.

\begin{equation}
\gamma = \frac{|s_1|^2 |s_2|^2}{\left( \frac{4-\pi}{2}\right) \left( |s_1|^2 \sigma_{n2}^2 + |s_2|^2 \sigma_{n1}^2 \right)}
\end{equation}


\begin{equation}
\gamma = \frac{2|s_1|^2 |s_2|^2}{\left( 4-\pi \right) \left( |s_1|^2 \sigma_{n2}^2 + |s_2|^2 \sigma_{n1}^2 \right)}
\end{equation}


\begin{equation}
\frac{1}{\gamma}
=
\frac{(4-\pi)(|s_1|^2 \sigma_{n2}^2)}{2|s_1|^2 |s_2|^2}
+
\frac{(4-\pi)(|s_2|^2\sigma_{n1}^2)}{2|s_1|^2 |s_2|^2}
\end{equation}


\begin{equation}
\frac{1}{\gamma}
=
\frac{(4-\pi)\sigma_{n2}^2}{|s_2|^2}
+
\frac{(4-\pi)\sigma_{n1}^2}{2|s_1|^2}
\end{equation}

\begin{equation}
\boxed{
\frac{1}{\gamma}
=
\frac{4-\pi}{2}
\left(
\frac{1}{\gamma_1} + \frac{1}{\gamma_2}
\right)
}
\end{equation}

The solution above is valid for the case where one of our correlated signals is clean, or has infinite SNR. Now let's go back to the case where the third term is not negligible. 

\begin{equation}
\gamma = \frac{|s_1 s_2|^2}{\Var(|s_1 n_2 + s_2 n_1 + n_1 n_2|)}
\end{equation}

Using the triangle inequality

\begin{equation}
\gamma \geq \frac{|s_1 s_2|^2}{\Var(|s_1 n_2 + s_2 n_1|) + \Var(|n_1 n_2|)}
\end{equation}

\begin{equation}
\gamma \geq \frac{|s_1 s_2|^2}{
\left(
\frac{4-\pi}{2}
\right)
\left(
|s_1|^2 \sigma_{n2}^2 + |s_2|^2 \sigma_{n1}^2
\right)
 +
0.38 \sigma_{n1}^2 \sigma_{n2}^2}
\end{equation}


\begin{equation}
\frac{1}{\gamma}
\leq
\frac{\left( 4-\pi \right) |s_1|^2 \sigma_{n2}^2}
{2|s_1|^2|s_2|^2}
+
\frac{\left( 4-\pi \right) |s_2|^2 \sigma_{n1}^2}
{2|s_1|^2|s_2|^2}
+
\frac{0.38\sigma_{n1}^2 \sigma_{n2}^2}
{|s_1|^2|s_2|^2}
\end{equation}


\begin{equation}
\frac{1}{\gamma}
\leq
\frac{\left( 4-\pi \right) \sigma_{n2}^2}
{2|s_2|^2}
+
\frac{\left( 4-\pi \right) \sigma_{n1}^2}
{2|s_1|^2}
+
\frac{0.38\sigma_{n1}^2 \sigma_{n2}^2}
{|s_1|^2|s_2|^2}
\end{equation}


\begin{equation}
\boxed{
\frac{1}{\gamma}
\leq
\frac{4-\pi}
{2 \gamma_1}
+
\frac{4-\pi}
{2 \gamma_2}
+
\frac{0.38}
{\gamma_1 \gamma_2}
}
\end{equation}


\section{Differential Matched Filter Detection}

Work in progress.

\begin{equation}
r = \sum s_1(t)s_1(t-1) s_2^*(t) s_2^*(t-1)
\end{equation}


\begin{equation}
\gamma
=
\frac{P_s}{P_n}
\end{equation}

\begin{equation}
P_s = |s_1(t)s_1(t-1)s_2(t)s_2(t-1)|^2
\end{equation}

\begin{align}
P_n = var(s_{11}s_{12}s_{21}n_{22} + s_{11}s_{12}s_{22}n_{21} + s_{11}s_{12}n_{21}n_{22}\\
  + s_{11}n_{12}s_{21}s_{22} + s_{11}n_{12}s_{21}n_{22} + s_{11}n_{12}s_{22}n_{21} + s_{11}n_{12}n_{21}n_{22}\\
  + s_{12}n_{11}s_{21}s_{22} + s_{12}n_{11}s_{21}n_{22} + s_{12}n_{11}s_{22}n_{21} + s_{12}n_{11}n_{21}n_{22}\\
  + n_{11}n_{12}s_{21}s_{22} + n_{11}n_{12}s_{21}n_{22} + n_{11}n_{12}s_{22}n_{21} + n_{11}n_{12}n_{21}n_{22})
\end{align}

Where $s_{11} = s_1(t)$, $s_{12} = s_1(t-1)$, $s_{21} = s_2(t)$, $s_{22} = s_2(t-1)$, $n_{11} = n_1(t)$, $n_{12} = n_1(t-1)$, $n_{21} = n_2(t)$, and $n_{22} = n_2(t-1)$.

%\begin{equation}
%s_{11} = s_1(t)
%\end{equation}

%\begin{equation}
%s_{12} = s_1(t-1)
%\end{equation}

%\begin{equation}
%s_{21} = s_2(t)
%\end{equation}

%\begin{equation}
%s_{22} = s_2(t-1)
%\end{equation}

%\begin{equation}
%n_{11} = n_1(t)
%\end{equation}

%\begin{equation}
%n_{12} = n_1(t-1)
%\end{equation}

%\begin{equation}
%n_{21} = n_2(t)
%\end{equation}

%\begin{equation}
%n_{22} = n_2(t-1)
%\end{equation}


This is probably easier to figure out experimentally. It is tempting to start aggressively canceling out terms that appear in both the numerator and the denominator. However, this would require invoking the Cauchy-Schwarz inequality.


\section{Concluding Thoughts}

Although these results allow us to calculate a post detection SNR, or at least a lower bound on SNR, we should be careful about using this value to calculate probability of detection or probability of false alarm metrics. The noise statistics are not purely Gaussian, or complex Gaussian, or Rayleigh distributed. When selecting a detection threshold, the actual probability distribution of the post detection data should be used.

In our results we make an implicit assumption that signal two and signal one are scaled versions of each other. That is $S_1(t) = \alpha s_2(t)$. This allows us to avoid the Cauchy-Schwarz inequality by guaranteeing it is actually an equality. The following simplification is made based on this assumption: $|s_1(t)s_2(t)|^2 = |s_1(t)|^2|s_2(t)|^2$. This assumption falls apart if $s_2(t)$ is not a scaled and rotated copy of $s_1(t)$.

The assumption also falls apart if there is a lag between $s_1(t)$ and $s_2(t)$. The work above deals with computing the output SNR for the time instant where both signals align perfectly. The implication of this is that Signal to Noise Ratio is not constant over a CAF. This should make intuitive sense given that the post detection signal magnitude is not constant over the CAF.

%--/Paper--

\end{document}
